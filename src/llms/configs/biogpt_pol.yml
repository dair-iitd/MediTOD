experiment_name: biogpt_pol
destpath: ./runs/biogpt_pol/
datapath: ../../data/pptod/
task: pol

model:
  wildcard: microsoft/biogpt
  model_max_length: 512
  use_trigger: True

train:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  gradient_checkpointing: false
  learning_rate: 0.0001
  num_epochs: 20
  seed: 42
  fp16: False
  save_eval_steps: 1
  evaluation_strategy: epoch
  save_strategy: epoch
  resume_training: True
  tf32: False
  save_total_limit: 2
  metric_for_best_model: accuracy
  greater_is_better: True
  early_stopping_patience: 5
  warmup_ratio: 0.1
  use_lora: False
  load_in_8bit: False
  lora_r: 256
  lora_alpha: 512
  lr_scheduler: "linear"

dev:
  per_device_eval_batch_size: 8
  sample: False
  num_beams: 1
  max_resp_length: 128
  top_k: 8
  top_p: 0.9
  temperature: 0.85

use_wandb: True