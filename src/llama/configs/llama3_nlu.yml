experiment_name: llama3_8b_ht4_nlu_templated
destpath: ./runs/llama3_8b_ht4_nlu_templated/
datapath: ../../data/pptod/
tasks:
  - nlu
history_size: 4
system_message:
  nlu: Given the dialog history and the last turn, identify the patient's intent, slots, and related attributes.

model:
  wildcard: meta-llama/Meta-Llama-3-8B-Instruct
  max_seq_length: 1024 # LLama3 tokenizer is high fidelity. So this value works
  quantization: 4

train:
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  learning_rate: 0.0001
  num_epochs: 10
  seed: 42
  fp16: false
  save_eval_steps: 512
  evaluation_strategy: steps
  save_strategy: steps
  save_total_limit: 2
  metric_for_best_model: loss
  greater_is_better: false
  early_stopping_patience: 5
  warmup_ratio: 0.1
  lr_scheduler: cosine

dev:
  per_device_eval_batch_size: 4

use_wandb: true